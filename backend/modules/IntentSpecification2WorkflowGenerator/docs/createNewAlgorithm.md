# Add specific engine support to an algorithm

## Information needed
* Implementation engine
* Mappings between the engine parameters and the "agnostic" ones defined on the base implementation
* Extra parameters required specific to the engine not possible to obtain from the "agnostic" parameters [optional]
* Specific information required for the translation step

## Steps
1. Choose one engine from [`ontology_populator/implementations/`](../ontology_populator/implementations/) or [implement a new one](./addNewEngine.md)
2. Create a new file: `ontology_populator/implementations/<chosen_engine>/<algorithm_name>.py`.
3. Create new engine implementations. You can use [knime/svm.py](../ontology_populator/implementations/knime/svm.py) as a reference. You need to specify a name, base implementation, engine parameters and other engine specific information.

<br>
Currently, there are three main parameter types:

* **Numeric parameters**: The mappings between the engine and "agnostic" parameters can be expressed through algebraic expressions using AlgebraicExpression. More details about algebraic operations [here](./algebraicExpression.md)
* **Text parameters**: mappings directly to an "agnostic" parameter
* **Specific parameters**: no mappings to an "agnostic" parameter. During the translation, only default values for this parameters will be used.



# Add a new algorithm to the ontology

## Information needed

* Algorithm name  (k-means, dbscan, etc.)
* Task that the algorithm solves (classification, clustering, normalization, etc.)
* Input objects (data, model, etc.)
* Input objects preconditions (normalized data, splitted, no null, etc.)
* Output objects (data, model, etc.)
* Output objects postconditions (normalized data, splitted, no null, etc.)
* General parameters (the most common ones across different implementations)

## Steps


1. Create a new file on: `ontology_populator/implementations/simple/<algorithm_name>.py`.
2. In that file create, at least, one implementation and one component. You can use an [decision_tree_py](../ontology_populator/implementations/simple/decision_tree.py) as a template. NOTE: Split shapes must be the first one defined among all preprocessing shapes. NOTE2: all the learner implementations should have the word Learner in the name field
    * When specifying input requirements, it is possible to add the importance level of each shape. The importance levels are:
        * Level 0 (**CRITICAL**): If the requirement is not met, the algorithm cannot be executed.
        * Level 1 (**MAJOR**): If the requriement is not met, the algorithm can be executed without problems, but the quality of the results produced is **severely** affected.
        * Level 2 (**MINOR**): If the requriement is not met, the algorithm can be executed without problems, but the quality of the results produced is **slightly** affected.
    
    The following code chunk shows an example of input shapes that an implementation could have. Input requirements can be instanciated using InputIOSpec class by specifying the requirements using IOSpecTag instances containing the shape and optionally its importance level. If the importance level is not specifiedm is assumed to be 0 (**Critical**).
    ```python
    input=[
        InputIOSpec([IOSpecTag(cb.TestTabularDatasetShape), IOSpecTag(cb.NonNullTabularDatasetShape,1)]), 
    ]
    ```

3. Update `ontology_populator/implementations/simple/__init__.py` with the added implementations and components
4. Modify [`ontology_populator/cbox_generator.py`](../ontology_populator/cbox_generator.py). You should update:
    * ```add_algorithms()```: Add the new algorithm referenced by the implementation and the problem it solves.
    * ```add_models()```: Add the new model types generated by the new algorithm, if any.
    * ```add_shapes()```: Add new shapes specified on the implementation and component functions if they are not present in the current ontology.
5. Execute the following command to regenerate the cbox:
    ```bash
    python cbox_generator.py
    ```

